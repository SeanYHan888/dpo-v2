model_name_or_path: Qwen/Qwen2-7B
beta: 0.1
lr: 5.0e-5
batch_size: 16
gradient_accumulation_steps: 1
num_epochs: 1
max_length: 512
fsdp:
  sharding_strategy: FULL_SHARD
  wrap_policy: TRANSFORMER_BASED_WRAP
  transformer_layer_cls: Qwen2DecoderLayer
  sync_module_states: true
